{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-opUIJ7wEEye"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1alUPNBbbgu3"
      },
      "outputs": [],
      "source": [
        "def set_seeds(seed: int=42):\n",
        "    \"\"\"Sets random sets for torch operations.\n",
        "\n",
        "    Args:\n",
        "        seed (int, optional): Random seed to set. Defaults to 42.\n",
        "    \"\"\"\n",
        "    # Set the seed for general torch operations\n",
        "    torch.manual_seed(seed)\n",
        "    # Set the seed for CUDA torch operations (ones that happen on the GPU)\n",
        "    torch.cuda.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dt5ULVFSbxFU"
      },
      "source": [
        "1. Get Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "GuHFt99zbwQ7",
        "outputId": "8704e48b-08d4-41f9-f8cb-0674439c40a7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HB7SUOXFb3hG",
        "outputId": "a1fa0ea0-3ff0-483b-9b50-4837e6057152"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5.00G/5.00G [02:40<00:00, 31.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "train_dataset = datasets.Food101(\n",
        "    root='/content',\n",
        "    split='train',\n",
        "    download=True,\n",
        "    transform=None  # No transform needed for sampling\n",
        ")\n",
        "\n",
        "test_dataset = datasets.Food101(\n",
        "    root='/content',\n",
        "    split='test',\n",
        "    download=True,\n",
        "    transform=None\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFk1WT-Ib5z_",
        "outputId": "4fb8ef90-324c-469e-b45e-95b837a8be00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sampling train split...\n",
            "  apple_pie: 375/750 images sampled\n",
            "  baby_back_ribs: 375/750 images sampled\n",
            "  baklava: 375/750 images sampled\n",
            "  beef_carpaccio: 375/750 images sampled\n",
            "  beef_tartare: 375/750 images sampled\n",
            "  beet_salad: 375/750 images sampled\n",
            "  beignets: 375/750 images sampled\n",
            "  bibimbap: 375/750 images sampled\n",
            "  bread_pudding: 375/750 images sampled\n",
            "  breakfast_burrito: 375/750 images sampled\n",
            "  bruschetta: 375/750 images sampled\n",
            "  caesar_salad: 375/750 images sampled\n",
            "  cannoli: 375/750 images sampled\n",
            "  caprese_salad: 375/750 images sampled\n",
            "  carrot_cake: 375/750 images sampled\n",
            "  ceviche: 375/750 images sampled\n",
            "  cheesecake: 375/750 images sampled\n",
            "  cheese_plate: 375/750 images sampled\n",
            "  chicken_curry: 375/750 images sampled\n",
            "  chicken_quesadilla: 375/750 images sampled\n",
            "  chicken_wings: 375/750 images sampled\n",
            "  chocolate_cake: 375/750 images sampled\n",
            "  chocolate_mousse: 375/750 images sampled\n",
            "  churros: 375/750 images sampled\n",
            "  clam_chowder: 375/750 images sampled\n",
            "  club_sandwich: 375/750 images sampled\n",
            "  crab_cakes: 375/750 images sampled\n",
            "  creme_brulee: 375/750 images sampled\n",
            "  croque_madame: 375/750 images sampled\n",
            "  cup_cakes: 375/750 images sampled\n",
            "  deviled_eggs: 375/750 images sampled\n",
            "  donuts: 375/750 images sampled\n",
            "  dumplings: 375/750 images sampled\n",
            "  edamame: 375/750 images sampled\n",
            "  eggs_benedict: 375/750 images sampled\n",
            "  escargots: 375/750 images sampled\n",
            "  falafel: 375/750 images sampled\n",
            "  filet_mignon: 375/750 images sampled\n",
            "  fish_and_chips: 375/750 images sampled\n",
            "  foie_gras: 375/750 images sampled\n",
            "  french_fries: 375/750 images sampled\n",
            "  french_onion_soup: 375/750 images sampled\n",
            "  french_toast: 375/750 images sampled\n",
            "  fried_calamari: 375/750 images sampled\n",
            "  fried_rice: 375/750 images sampled\n",
            "  frozen_yogurt: 375/750 images sampled\n",
            "  garlic_bread: 375/750 images sampled\n",
            "  gnocchi: 375/750 images sampled\n",
            "  greek_salad: 375/750 images sampled\n",
            "  grilled_cheese_sandwich: 375/750 images sampled\n",
            "  grilled_salmon: 375/750 images sampled\n",
            "  guacamole: 375/750 images sampled\n",
            "  gyoza: 375/750 images sampled\n",
            "  hamburger: 375/750 images sampled\n",
            "  hot_and_sour_soup: 375/750 images sampled\n",
            "  hot_dog: 375/750 images sampled\n",
            "  huevos_rancheros: 375/750 images sampled\n",
            "  hummus: 375/750 images sampled\n",
            "  ice_cream: 375/750 images sampled\n",
            "  lasagna: 375/750 images sampled\n",
            "  lobster_bisque: 375/750 images sampled\n",
            "  lobster_roll_sandwich: 375/750 images sampled\n",
            "  macaroni_and_cheese: 375/750 images sampled\n",
            "  macarons: 375/750 images sampled\n",
            "  miso_soup: 375/750 images sampled\n",
            "  mussels: 375/750 images sampled\n",
            "  nachos: 375/750 images sampled\n",
            "  omelette: 375/750 images sampled\n",
            "  onion_rings: 375/750 images sampled\n",
            "  oysters: 375/750 images sampled\n",
            "  pad_thai: 375/750 images sampled\n",
            "  paella: 375/750 images sampled\n",
            "  pancakes: 375/750 images sampled\n",
            "  panna_cotta: 375/750 images sampled\n",
            "  peking_duck: 375/750 images sampled\n",
            "  pho: 375/750 images sampled\n",
            "  pizza: 375/750 images sampled\n",
            "  pork_chop: 375/750 images sampled\n",
            "  poutine: 375/750 images sampled\n",
            "  prime_rib: 375/750 images sampled\n",
            "  pulled_pork_sandwich: 375/750 images sampled\n",
            "  ramen: 375/750 images sampled\n",
            "  ravioli: 375/750 images sampled\n",
            "  red_velvet_cake: 375/750 images sampled\n",
            "  risotto: 375/750 images sampled\n",
            "  samosa: 375/750 images sampled\n",
            "  sashimi: 375/750 images sampled\n",
            "  scallops: 375/750 images sampled\n",
            "  seaweed_salad: 375/750 images sampled\n",
            "  shrimp_and_grits: 375/750 images sampled\n",
            "  spaghetti_bolognese: 375/750 images sampled\n",
            "  spaghetti_carbonara: 375/750 images sampled\n",
            "  spring_rolls: 375/750 images sampled\n",
            "  steak: 375/750 images sampled\n",
            "  strawberry_shortcake: 375/750 images sampled\n",
            "  sushi: 375/750 images sampled\n",
            "  tacos: 375/750 images sampled\n",
            "  takoyaki: 375/750 images sampled\n",
            "  tiramisu: 375/750 images sampled\n",
            "  tuna_tartare: 375/750 images sampled\n",
            "  waffles: 375/750 images sampled\n",
            "\n",
            "Sampling test split...\n",
            "  apple_pie: 125/250 images sampled\n",
            "  baby_back_ribs: 125/250 images sampled\n",
            "  baklava: 125/250 images sampled\n",
            "  beef_carpaccio: 125/250 images sampled\n",
            "  beef_tartare: 125/250 images sampled\n",
            "  beet_salad: 125/250 images sampled\n",
            "  beignets: 125/250 images sampled\n",
            "  bibimbap: 125/250 images sampled\n",
            "  bread_pudding: 125/250 images sampled\n",
            "  breakfast_burrito: 125/250 images sampled\n",
            "  bruschetta: 125/250 images sampled\n",
            "  caesar_salad: 125/250 images sampled\n",
            "  cannoli: 125/250 images sampled\n",
            "  caprese_salad: 125/250 images sampled\n",
            "  carrot_cake: 125/250 images sampled\n",
            "  ceviche: 125/250 images sampled\n",
            "  cheesecake: 125/250 images sampled\n",
            "  cheese_plate: 125/250 images sampled\n",
            "  chicken_curry: 125/250 images sampled\n",
            "  chicken_quesadilla: 125/250 images sampled\n",
            "  chicken_wings: 125/250 images sampled\n",
            "  chocolate_cake: 125/250 images sampled\n",
            "  chocolate_mousse: 125/250 images sampled\n",
            "  churros: 125/250 images sampled\n",
            "  clam_chowder: 125/250 images sampled\n",
            "  club_sandwich: 125/250 images sampled\n",
            "  crab_cakes: 125/250 images sampled\n",
            "  creme_brulee: 125/250 images sampled\n",
            "  croque_madame: 125/250 images sampled\n",
            "  cup_cakes: 125/250 images sampled\n",
            "  deviled_eggs: 125/250 images sampled\n",
            "  donuts: 125/250 images sampled\n",
            "  dumplings: 125/250 images sampled\n",
            "  edamame: 125/250 images sampled\n",
            "  eggs_benedict: 125/250 images sampled\n",
            "  escargots: 125/250 images sampled\n",
            "  falafel: 125/250 images sampled\n",
            "  filet_mignon: 125/250 images sampled\n",
            "  fish_and_chips: 125/250 images sampled\n",
            "  foie_gras: 125/250 images sampled\n",
            "  french_fries: 125/250 images sampled\n",
            "  french_onion_soup: 125/250 images sampled\n",
            "  french_toast: 125/250 images sampled\n",
            "  fried_calamari: 125/250 images sampled\n",
            "  fried_rice: 125/250 images sampled\n",
            "  frozen_yogurt: 125/250 images sampled\n",
            "  garlic_bread: 125/250 images sampled\n",
            "  gnocchi: 125/250 images sampled\n",
            "  greek_salad: 125/250 images sampled\n",
            "  grilled_cheese_sandwich: 125/250 images sampled\n",
            "  grilled_salmon: 125/250 images sampled\n",
            "  guacamole: 125/250 images sampled\n",
            "  gyoza: 125/250 images sampled\n",
            "  hamburger: 125/250 images sampled\n",
            "  hot_and_sour_soup: 125/250 images sampled\n",
            "  hot_dog: 125/250 images sampled\n",
            "  huevos_rancheros: 125/250 images sampled\n",
            "  hummus: 125/250 images sampled\n",
            "  ice_cream: 125/250 images sampled\n",
            "  lasagna: 125/250 images sampled\n",
            "  lobster_bisque: 125/250 images sampled\n",
            "  lobster_roll_sandwich: 125/250 images sampled\n",
            "  macaroni_and_cheese: 125/250 images sampled\n",
            "  macarons: 125/250 images sampled\n",
            "  miso_soup: 125/250 images sampled\n",
            "  mussels: 125/250 images sampled\n",
            "  nachos: 125/250 images sampled\n",
            "  omelette: 125/250 images sampled\n",
            "  onion_rings: 125/250 images sampled\n",
            "  oysters: 125/250 images sampled\n",
            "  pad_thai: 125/250 images sampled\n",
            "  paella: 125/250 images sampled\n",
            "  pancakes: 125/250 images sampled\n",
            "  panna_cotta: 125/250 images sampled\n",
            "  peking_duck: 125/250 images sampled\n",
            "  pho: 125/250 images sampled\n",
            "  pizza: 125/250 images sampled\n",
            "  pork_chop: 125/250 images sampled\n",
            "  poutine: 125/250 images sampled\n",
            "  prime_rib: 125/250 images sampled\n",
            "  pulled_pork_sandwich: 125/250 images sampled\n",
            "  ramen: 125/250 images sampled\n",
            "  ravioli: 125/250 images sampled\n",
            "  red_velvet_cake: 125/250 images sampled\n",
            "  risotto: 125/250 images sampled\n",
            "  samosa: 125/250 images sampled\n",
            "  sashimi: 125/250 images sampled\n",
            "  scallops: 125/250 images sampled\n",
            "  seaweed_salad: 125/250 images sampled\n",
            "  shrimp_and_grits: 125/250 images sampled\n",
            "  spaghetti_bolognese: 125/250 images sampled\n",
            "  spaghetti_carbonara: 125/250 images sampled\n",
            "  spring_rolls: 125/250 images sampled\n",
            "  steak: 125/250 images sampled\n",
            "  strawberry_shortcake: 125/250 images sampled\n",
            "  sushi: 125/250 images sampled\n",
            "  tacos: 125/250 images sampled\n",
            "  takoyaki: 125/250 images sampled\n",
            "  tiramisu: 125/250 images sampled\n",
            "  tuna_tartare: 125/250 images sampled\n",
            "  waffles: 125/250 images sampled\n",
            "\n",
            "Sampled dataset created at /content/food101_sampled\n",
            "apple_pie\n",
            "baby_back_ribs\n",
            "baklava\n",
            "beef_carpaccio\n",
            "beef_tartare\n",
            "\n",
            "Total sampled images:\n",
            "50500\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "from torchvision import datasets\n",
        "\n",
        "# Step 4: Create the sampled directory structure\n",
        "sampled_root = Path('/content/food101_sampled')\n",
        "sampled_train = sampled_root / 'train'\n",
        "sampled_test = sampled_root / 'test'\n",
        "sampled_train.mkdir(parents=True, exist_ok=True)\n",
        "sampled_test.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Step 5: Sample 10% of images per category using metadata\n",
        "sample_fraction = 0.5  # 10%\n",
        "base_image_path = Path('/content/food-101/images')\n",
        "meta_path = Path('/content/food-101/meta')\n",
        "\n",
        "# Read train and test metadata\n",
        "train_images = {}\n",
        "test_images = {}\n",
        "\n",
        "# Read train.txt with error handling\n",
        "with open(meta_path / 'train.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:  # Skip empty lines\n",
        "            continue\n",
        "        parts = line.split(' ')\n",
        "        if len(parts) < 1:  # Skip malformed lines\n",
        "            print(f\"Warning: Skipping malformed line in train.txt: {line}\")\n",
        "            continue\n",
        "        img_path = parts[0]  # e.g., apple_pie/12345\n",
        "        class_name = img_path.split('/')[0]\n",
        "        full_path = base_image_path / f\"{img_path}.jpg\"\n",
        "        if class_name not in train_images:\n",
        "            train_images[class_name] = []\n",
        "        train_images[class_name].append(str(full_path))\n",
        "\n",
        "# Read test.txt with error handling\n",
        "with open(meta_path / 'test.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:  # Skip empty lines\n",
        "            continue\n",
        "        parts = line.split(' ')\n",
        "        if len(parts) < 1:  # Skip malformed lines\n",
        "            print(f\"Warning: Skipping malformed line in test.txt: {line}\")\n",
        "            continue\n",
        "        img_path = parts[0]  # e.g., apple_pie/12345\n",
        "        class_name = img_path.split('/')[0]\n",
        "        full_path = base_image_path / f\"{img_path}.jpg\"\n",
        "        if class_name not in test_images:\n",
        "            test_images[class_name] = []\n",
        "        test_images[class_name].append(str(full_path))\n",
        "\n",
        "# Sample and copy images\n",
        "for split, image_dict, sampled_dir in [\n",
        "    ('train', train_images, sampled_train),\n",
        "    ('test', test_images, sampled_test)\n",
        "]:\n",
        "    print(f\"\\nSampling {split} split...\")\n",
        "    for class_name, img_paths in image_dict.items():\n",
        "        num_images = len(img_paths)\n",
        "        num_sample = max(1, int(num_images * sample_fraction))  # At least 1 per category\n",
        "        sampled_paths = random.sample(img_paths, num_sample)\n",
        "\n",
        "        # Create category subdir\n",
        "        class_dir = sampled_dir / class_name\n",
        "        class_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # Copy images\n",
        "        for src_path in sampled_paths:\n",
        "            filename = Path(src_path).name\n",
        "            dst_path = class_dir / filename\n",
        "            if Path(src_path).exists():\n",
        "                shutil.copy2(src_path, dst_path)\n",
        "            else:\n",
        "                print(f\"Warning: {src_path} not found\")\n",
        "\n",
        "        print(f\"  {class_name}: {num_sample}/{num_images} images sampled\")\n",
        "\n",
        "# Step 6: Verify the sampled directory\n",
        "print(f\"\\nSampled dataset created at /content/food101_sampled\")\n",
        "!ls /content/food101_sampled/train | head -5  # Show first 5 categories in train\n",
        "print(f\"\\nTotal sampled images:\")\n",
        "!find /content/food101_sampled -type f | wc -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYIpvOGCB4fD"
      },
      "outputs": [],
      "source": [
        "height = 224 #\n",
        "width = 224\n",
        "color_channels = 3\n",
        "patch_size = 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQUI5u0ubly0"
      },
      "source": [
        "Equation 1: Split data into patches and creating the class, position and patch embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlaE_kaVoxrU"
      },
      "outputs": [],
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "  def __init__(self,\n",
        "              in_channels:int=3,\n",
        "              patch_size:int=16,\n",
        "              embedding_dim:int=768):\n",
        "    super().__init__()\n",
        "\n",
        "    self.patcher = nn.Conv2d(in_channels=color_channels,\n",
        "                         out_channels=embedding_dim,\n",
        "                         kernel_size=patch_size,\n",
        "                         stride=patch_size,\n",
        "                         padding=0)\n",
        "    self.flatten = nn.Flatten(start_dim=2, end_dim=3)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "      # Create assertion to check that inputs are the correct shape\n",
        "      image_resolution = x.shape[-1]\n",
        "      assert image_resolution % patch_size == 0, f\"Input image size must be divisible by patch size, image shape: {image_resolution}, patch size: {patch_size}\"\n",
        "\n",
        "      # Perform the forward pass\n",
        "      x_patched = self.patcher(x)\n",
        "      x_flattened = self.flatten(x_patched)\n",
        "      # 6. Make sure the output shape has the right order\n",
        "      return x_flattened.permute(0, 2, 1) # adjust so the embedding is on the final dimension [batch_size, P^2•C, N] -> [batch_size, N, P^2•C]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "qZCtRsX5cJqZ",
        "outputId": "e44f1615-2763-46a4-91af-61c852cc39a1"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'batch_size' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4229901320.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create the class token embedding as a learnable parameter that shares the same size as the embedding dimension (D)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension), # [batch_size, number_of_tokens, embedding_dimension]\n\u001b[0m\u001b[1;32m      3\u001b[0m                            requires_grad=True)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
          ]
        }
      ],
      "source": [
        "# Create the class token embedding as a learnable parameter that shares the same size as the embedding dimension (D)\n",
        "class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension), # [batch_size, number_of_tokens, embedding_dimension]\n",
        "                           requires_grad=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9y_h9xh7cKxZ"
      },
      "outputs": [],
      "source": [
        "# Create the learnable 1D position embedding\n",
        "position_embedding = nn.Parameter(torch.ones(1,\n",
        "                                             number_of_patches+1,\n",
        "                                             embedding_dimension),\n",
        "                                  requires_grad=True) # make sure it's learnable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpSpvW8xcST6"
      },
      "source": [
        "Equation 2: Multi-Head Attention (MSA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnEZeaSbJ6EB"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttentionBlock(nn.Module):\n",
        "  def __init__(self,\n",
        "              embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
        "              num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
        "              attn_dropout:float=0):\n",
        "    super().__init__()\n",
        "\n",
        "    self.Layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
        "\n",
        "    self.MultiHeadAttention = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
        "                                                    num_heads=num_heads,\n",
        "                                                    dropout=attn_dropout,\n",
        "                                                    batch_first=True)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.Layer_norm(x)\n",
        "    return self.MultiHeadAttention(query=x,\n",
        "                                   key=x,\n",
        "                                   value=x)[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4RtFFKhcY83"
      },
      "source": [
        " Equation 3: Multilayer Perceptron (MLP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUinLxjuKqmM"
      },
      "outputs": [],
      "source": [
        "class MLPBlock(nn.Module):\n",
        "  def __init__(self,embedding_dim:int=768, # Hidden Size D from Table 1 for ViT-Base\n",
        "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
        "                 dropout:float=0.1):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
        "\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(in_features=embedding_dim,\n",
        "                      out_features=mlp_size),\n",
        "        nn.GELU(), # \"The MLP contains two layers with a GELU non-linearity (section 3.1).\"\n",
        "        nn.Dropout(p=dropout),\n",
        "        nn.Linear(in_features=mlp_size, # needs to take same in_features as out_features of layer above\n",
        "                      out_features=embedding_dim), # take back to embedding_dim\n",
        "        nn.Dropout(p=dropout))\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.mlp(self.layer_norm(x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgW9h-agceRj"
      },
      "source": [
        "Create the Transformer Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrdeJIFZLklD"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderBlock(nn.Module):\n",
        "  def __init__(self,\n",
        "              embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
        "              num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
        "              mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
        "              mlp_dropout:float=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base\n",
        "              attn_dropout:float=0):\n",
        "    super().__init__()\n",
        "\n",
        "    self.MultiHeadSelfAttentionBlock = MultiHeadSelfAttentionBlock(embedding_dim=embedding_dim,\n",
        "                                                                   num_heads=num_heads,\n",
        "                                                                   attn_dropout=attn_dropout)\n",
        "    self.MLPBlock = MLPBlock(embedding_dim=embedding_dim,\n",
        "                             mlp_size=mlp_size,\n",
        "                             dropout=mlp_dropout)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.MultiHeadSelfAttentionBlock(x) + x\n",
        "    x = self.MLPBlock(x) + x\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vvpg2ZIOM_Wt",
        "outputId": "584f5635-3965-478e-f98f-689fc788b87e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TransformerEncoderLayer(\n",
              "  (self_attn): MultiheadAttention(\n",
              "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "  )\n",
              "  (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "  (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  (dropout1): Dropout(p=0.1, inplace=False)\n",
              "  (dropout2): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create the same as above with torch.nn.TransformerEncoderLayer()\n",
        "torch_transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=768, # Hidden size D from Table 1 for ViT-Base\n",
        "                                                             nhead=12, # Heads from Table 1 for ViT-Base\n",
        "                                                             dim_feedforward=3072, # MLP size from Table 1 for ViT-Base\n",
        "                                                             dropout=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base\n",
        "                                                             activation=\"gelu\", # GELU non-linear activation\n",
        "                                                             batch_first=True, # Do our batches come first?\n",
        "                                                             norm_first=True) # Normalize first or after MSA/MLP layers?\n",
        "\n",
        "torch_transformer_encoder_layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZH4ytCtrNGqg"
      },
      "source": [
        "**Putting it all together to create ViT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRxpJJ-fNJw3"
      },
      "outputs": [],
      "source": [
        "class ViT(nn.Module):\n",
        "  def __init__(self,\n",
        "                img_size:int=224, # Training resolution from Table 3 in ViT paper\n",
        "                in_channels:int=3, # Number of channels in input image\n",
        "                patch_size:int=16, # Patch size\n",
        "                num_transformer_layers:int=12, # Layers from Table 1 for ViT-Base\n",
        "                embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
        "                mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
        "                num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
        "                attn_dropout:float=0, # Dropout for attention projection\n",
        "                mlp_dropout:float=0.1, # Dropout for dense/MLP layers\n",
        "                embedding_dropout:float=0.1, # Dropout for patch and position embeddings\n",
        "                num_classes:int=1000):\n",
        "    super().__init__()\n",
        "\n",
        "    # 3. Make the image size is divisible by the patch size\n",
        "    assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"\n",
        "\n",
        "    # 4. Calculate number of patches (height * width/patch^2)\n",
        "    self.num_patches = (img_size * img_size) // patch_size**2\n",
        "\n",
        "    self.class_embedding = nn.Parameter(data=torch.randn(1,1,embedding_dim),requires_grad=True)\n",
        "\n",
        "    self.position_embedding = nn.Parameter(data=torch.randn(1,self.num_patches+1,embedding_dim),requires_grad=True)\n",
        "\n",
        "    self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
        "    self.patch_embedding = PatchEmbedding(in_channels=in_channels,\n",
        "                                          patch_size=patch_size,\n",
        "                                          embedding_dim=embedding_dim)\n",
        "\n",
        "    self.TransformerEncoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n",
        "                                                                      num_heads=num_heads,\n",
        "                                                                      mlp_size=mlp_size,\n",
        "                                                                      mlp_dropout=mlp_dropout,\n",
        "                                                                      attn_dropout=attn_dropout)for _ in range(num_transformer_layers)])\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(normalized_shape=embedding_dim),\n",
        "            nn.Linear(in_features=embedding_dim,\n",
        "                      out_features=num_classes)\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "        # 12. Get batch size\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        # 13. Create class token embedding and expand it to match the batch size (equation 1)\n",
        "        class_token = self.class_embedding.expand(batch_size, -1, -1) # \"-1\" means to infer the dimension (try this line on its own)\n",
        "\n",
        "        # 14. Create patch embedding (equation 1)\n",
        "        x = self.patch_embedding(x)\n",
        "\n",
        "        # 15. Concat class embedding and patch embedding (equation 1)\n",
        "        x = torch.cat((class_token, x), dim=1)\n",
        "\n",
        "        # 16. Add position embedding to patch embedding (equation 1)\n",
        "        x = self.position_embedding + x\n",
        "\n",
        "        # 17. Run embedding dropout (Appendix B.1)\n",
        "        x = self.embedding_dropout(x)\n",
        "\n",
        "        # 18. Pass patch, position and class embedding through transformer encoder layers (equations 2 & 3)\n",
        "        x = self.TransformerEncoder(x)\n",
        "\n",
        "        # 19. Put 0 index logit through classifier (equation 4)\n",
        "        x = self.classifier(x[:, 0]) # run on each sample in a batch at 0 index\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxJFQMTXfSeK"
      },
      "outputs": [],
      "source": [
        "set_seeds()\n",
        "vit = ViT(num_classes=101)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uj4clw1ij7FV",
        "outputId": "363c0873-5a2a-44cd-c043-6224e5615fd1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7c6ba3b9d0a0>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7c6ba4656c00>,\n",
              " ['apple_pie',\n",
              "  'baby_back_ribs',\n",
              "  'baklava',\n",
              "  'beef_carpaccio',\n",
              "  'beef_tartare',\n",
              "  'beet_salad',\n",
              "  'beignets',\n",
              "  'bibimbap',\n",
              "  'bread_pudding',\n",
              "  'breakfast_burrito',\n",
              "  'bruschetta',\n",
              "  'caesar_salad',\n",
              "  'cannoli',\n",
              "  'caprese_salad',\n",
              "  'carrot_cake',\n",
              "  'ceviche',\n",
              "  'cheese_plate',\n",
              "  'cheesecake',\n",
              "  'chicken_curry',\n",
              "  'chicken_quesadilla',\n",
              "  'chicken_wings',\n",
              "  'chocolate_cake',\n",
              "  'chocolate_mousse',\n",
              "  'churros',\n",
              "  'clam_chowder',\n",
              "  'club_sandwich',\n",
              "  'crab_cakes',\n",
              "  'creme_brulee',\n",
              "  'croque_madame',\n",
              "  'cup_cakes',\n",
              "  'deviled_eggs',\n",
              "  'donuts',\n",
              "  'dumplings',\n",
              "  'edamame',\n",
              "  'eggs_benedict',\n",
              "  'escargots',\n",
              "  'falafel',\n",
              "  'filet_mignon',\n",
              "  'fish_and_chips',\n",
              "  'foie_gras',\n",
              "  'french_fries',\n",
              "  'french_onion_soup',\n",
              "  'french_toast',\n",
              "  'fried_calamari',\n",
              "  'fried_rice',\n",
              "  'frozen_yogurt',\n",
              "  'garlic_bread',\n",
              "  'gnocchi',\n",
              "  'greek_salad',\n",
              "  'grilled_cheese_sandwich',\n",
              "  'grilled_salmon',\n",
              "  'guacamole',\n",
              "  'gyoza',\n",
              "  'hamburger',\n",
              "  'hot_and_sour_soup',\n",
              "  'hot_dog',\n",
              "  'huevos_rancheros',\n",
              "  'hummus',\n",
              "  'ice_cream',\n",
              "  'lasagna',\n",
              "  'lobster_bisque',\n",
              "  'lobster_roll_sandwich',\n",
              "  'macaroni_and_cheese',\n",
              "  'macarons',\n",
              "  'miso_soup',\n",
              "  'mussels',\n",
              "  'nachos',\n",
              "  'omelette',\n",
              "  'onion_rings',\n",
              "  'oysters',\n",
              "  'pad_thai',\n",
              "  'paella',\n",
              "  'pancakes',\n",
              "  'panna_cotta',\n",
              "  'peking_duck',\n",
              "  'pho',\n",
              "  'pizza',\n",
              "  'pork_chop',\n",
              "  'poutine',\n",
              "  'prime_rib',\n",
              "  'pulled_pork_sandwich',\n",
              "  'ramen',\n",
              "  'ravioli',\n",
              "  'red_velvet_cake',\n",
              "  'risotto',\n",
              "  'samosa',\n",
              "  'sashimi',\n",
              "  'scallops',\n",
              "  'seaweed_salad',\n",
              "  'shrimp_and_grits',\n",
              "  'spaghetti_bolognese',\n",
              "  'spaghetti_carbonara',\n",
              "  'spring_rolls',\n",
              "  'steak',\n",
              "  'strawberry_shortcake',\n",
              "  'sushi',\n",
              "  'tacos',\n",
              "  'takoyaki',\n",
              "  'tiramisu',\n",
              "  'tuna_tartare',\n",
              "  'waffles'])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import data_setup\n",
        "test_dir = '/content/food101_sampled/test'\n",
        "train_dir = '/content/food101_sampled/train'\n",
        "BATCH_SIZE = 32 # this is lower than the ViT paper but it's because we're starting small\n",
        "\n",
        "# Create image size (from Table 3 in the ViT paper)\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# Create transform pipeline manually\n",
        "manual_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Create data loaders\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=manual_transforms, # use manually created transforms\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "train_dataloader, test_dataloader, class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "f425d11c6c2a44988b2cb00afb87c22e",
            "975e28d4faec4f058d3eccc55a8af99d",
            "0f37430f8a2548c0a5cd3c85bd38296e",
            "54c82c1d406c49efb03d333a3dd7eef0",
            "4453d714a8434008bbde42ca73f20c30",
            "b80492daba054dc8a3ae4cd4cbccf717",
            "9aa54209b9e64a1480b621d057dcdc31",
            "021320bc96184b62b4115044ea9b4efb",
            "ed089392953644d1bbcc8956d3d3a530",
            "9727b41f461a4969aeb790fadc57b09e",
            "32745fc8ecd04f449e7e1475112639c8"
          ]
        },
        "id": "y5MSrLEWjpXG",
        "outputId": "c7ad6993-2c8e-4464-8544-b18dce0aeefe"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f425d11c6c2a44988b2cb00afb87c22e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 | train_loss: 4.6839 | train_acc: 0.0100 | test_loss: 4.6152 | test_acc: 0.0099\n"
          ]
        }
      ],
      "source": [
        "import engine\n",
        "\n",
        "# Setup the optimizer to optimize our ViT model parameters using hyperparameters from the ViT paper\n",
        "optimizer = torch.optim.Adam(params=vit.parameters(),\n",
        "                             lr=3e-3, # Base LR from Table 3 for ViT-* ImageNet-1k\n",
        "                             betas=(0.9, 0.999), # default values but also mentioned in ViT paper section 4.1 (Training & Fine-tuning)\n",
        "                             weight_decay=0.3) # from the ViT paper section 4.1 (Training & Fine-tuning) and Table 3 for ViT-* ImageNet-1k\n",
        "\n",
        "# Setup the loss function for multi-class classification\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Set the seeds\n",
        "set_seeds()\n",
        "\n",
        "results = engine.train(model=vit,\n",
        "                       train_dataloader=train_dataloader,\n",
        "                       test_dataloader=test_dataloader,\n",
        "                       optimizer=optimizer,\n",
        "                       loss_fn=loss_fn,\n",
        "                       epochs=10,\n",
        "                       device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03d74496"
      },
      "outputs": [],
      "source": [
        "from utils import plot_loss_curves\n",
        "\n",
        "plot_loss_curves(results)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "021320bc96184b62b4115044ea9b4efb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f37430f8a2548c0a5cd3c85bd38296e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_021320bc96184b62b4115044ea9b4efb",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ed089392953644d1bbcc8956d3d3a530",
            "value": 0
          }
        },
        "32745fc8ecd04f449e7e1475112639c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4453d714a8434008bbde42ca73f20c30": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54c82c1d406c49efb03d333a3dd7eef0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9727b41f461a4969aeb790fadc57b09e",
            "placeholder": "​",
            "style": "IPY_MODEL_32745fc8ecd04f449e7e1475112639c8",
            "value": " 0/10 [00:00&lt;?, ?it/s]"
          }
        },
        "9727b41f461a4969aeb790fadc57b09e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "975e28d4faec4f058d3eccc55a8af99d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b80492daba054dc8a3ae4cd4cbccf717",
            "placeholder": "​",
            "style": "IPY_MODEL_9aa54209b9e64a1480b621d057dcdc31",
            "value": "  0%"
          }
        },
        "9aa54209b9e64a1480b621d057dcdc31": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b80492daba054dc8a3ae4cd4cbccf717": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed089392953644d1bbcc8956d3d3a530": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f425d11c6c2a44988b2cb00afb87c22e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_975e28d4faec4f058d3eccc55a8af99d",
              "IPY_MODEL_0f37430f8a2548c0a5cd3c85bd38296e",
              "IPY_MODEL_54c82c1d406c49efb03d333a3dd7eef0"
            ],
            "layout": "IPY_MODEL_4453d714a8434008bbde42ca73f20c30"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}